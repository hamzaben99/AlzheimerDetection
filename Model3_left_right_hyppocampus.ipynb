{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Alzeihmer desease detection using volume patches ","metadata":{"execution":{"iopub.status.busy":"2021-12-09T14:54:11.834135Z","iopub.execute_input":"2021-12-09T14:54:11.834403Z","iopub.status.idle":"2021-12-09T14:54:11.838212Z","shell.execute_reply.started":"2021-12-09T14:54:11.834374Z","shell.execute_reply":"2021-12-09T14:54:11.837259Z"}}},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom nibabel.testing import data_path\nimport nibabel as nib\nfrom pathlib import Path\n\npath = \"kaggle/input/adni-data\"\npath = path + \"\\ADNI_PROCESSED\"\n\ndef apply_mask(img_n_mmni, img_mask):\n    \"\"\"\n        Taking a n_mmni and apply the correspondant mask\n        param:\n            img_n_mmi   : image n_mmi\n            img_mask    : mask\n    \"\"\"\n    mmni_m = img_n_mmni.get_fdata()\n    mask_m = img_mask.get_fdata().astype(bool)\n    mask_bg = np.logical_not(mask_m)\n    mmni_m[mask_bg] = 0\n    return mmni_m\n\ndef process_irm_data():\n    \"\"\"\n        Create a new directory and process all images from tha ADNI1 directory\n    \"\"\"\n    path = str(Path().resolve())\n    path_res = path + \"\\\\ADNI_PROCESSED\"\n    Path(path_res).mkdir(parents=True, exist_ok=True) # Create a directory for data processed\n    path = path + \"\\\\ADNI1\"\n    for filename in os.listdir(path):\n        if filename.startswith(\"n_mmni\"):\n            n_mmni_filename = os.path.join(path, filename)\n            mask_filename = os.path.join(path, \"mask_\" + filename)\n            img_n_mmni = nib.load(n_mmni_filename)\n            img_mask = nib.load(mask_filename)\n            n_mmni_mask = apply_mask(img_n_mmni, img_mask)\n            img = nib.Nifti1Image(n_mmni_mask, np.eye(4))\n            nib.save(img, os.path.join(path_res, filename))","metadata":{"execution":{"iopub.status.busy":"2021-12-09T14:45:05.264343Z","iopub.execute_input":"2021-12-09T14:45:05.264653Z","iopub.status.idle":"2021-12-09T14:45:05.777364Z","shell.execute_reply.started":"2021-12-09T14:45:05.264570Z","shell.execute_reply":"2021-12-09T14:45:05.776561Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def cut_2D_i(img_n_mmni, axe, idx):\n    \"\"\"\n        Function that returns a 2D cut from the \"img\" in the index \"idx\", along the axe given in parameter\n    \"\"\"\n    axe_dim = {\"x\": img_n_mmni.shape[0], \"y\": img_n_mmni.shape[1], \"z\":img_n_mmni.shape[2]}\n    if axe_dim[axe] <= idx or idx < 0:\n        print(\"Invalid value for index must be between 0 and \" , axe_dim[axe])\n        return\n    if axe == \"x\":\n        cropped_img = img_n_mmni.slicer[idx:idx+1, 18:199, :]\n        img_data = cropped_img.get_fdata()\n        img_data = np.transpose(img_data, (2, 1, 0)) / 255.0\n        img_data = np.transpose(img_data, (1, 0, 2)) / 255.0\n    elif axe == \"y\":\n        cropped_img = img_n_mmni.slicer[45:145, idx:idx+1,35:135] #eliminate black borders\n        img_data = cropped_img.get_fdata()\n        img_data = np.transpose(img_data, (0, 2, 1)) / 255.0\n    elif axe == \"z\":\n        cropped_img = img_n_mmni.slicer[..., idx:idx+1]\n        img_data = cropped_img.get_fdata()\n    else:\n        print(\"Choose a valid value for axe: x, y or z\")\n\n    return img_data\n\ndef custom_patch_3D(img_n_mmni, x_tup, y_tup, z_tup):\n    axe_dim = {\"x\": img_n_mmni.shape[0], \"y\": img_n_mmni.shape[1], \"z\":img_n_mmni.shape[2]}\n    if axe_dim[\"x\"] <= x_tup[1] or x_tup[0] < 0 or axe_dim[\"y\"] <= y_tup[1] or y_tup[0] < 0 or axe_dim[\"z\"] <= z_tup[1] or z_tup[0] < 0 :\n        print(\"Invalid values\")\n        return \n    else:\n        cropped_img = img_n_mmni.slicer[x_tup[0]:x_tup[1], y_tup[0]:y_tup[1], z_tup[0]:z_tup[1]]\n        img_data = cropped_img.get_fdata()\n        img_data.resize(img_data.shape + (1, ))\n        return img_data","metadata":{"execution":{"iopub.status.busy":"2021-12-09T14:45:08.564503Z","iopub.execute_input":"2021-12-09T14:45:08.564781Z","iopub.status.idle":"2021-12-09T14:45:08.576909Z","shell.execute_reply.started":"2021-12-09T14:45:08.564750Z","shell.execute_reply":"2021-12-09T14:45:08.576076Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import os\nimport re\nfrom pathlib import Path\nimport pandas as pd\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(path):\n    data = pd.read_csv(path, names= ['Subject ID', 'Rooster ID', 'Age', 'Sexe', 'Group', 'Conversion', 'MMSE', 'RAVLT', 'FAQ', 'CDR-SB', 'ADAS11'], usecols = ['Subject ID', 'Rooster ID', 'Group'])\n    data.index = data['Subject ID']\n    data = data.drop(['Subject ID'], axis=1)\n    data = data[(data.Group == 'CN') | (data.Group == 'AD')]\n    return data\n\npath = \"../input/adni-data\"\npath = path + \"/list_standardized_tongtong_2017.csv\"\ny_data = load_data(path)\ny_data.head(7)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T14:45:11.740211Z","iopub.execute_input":"2021-12-09T14:45:11.740746Z","iopub.status.idle":"2021-12-09T14:45:12.561123Z","shell.execute_reply.started":"2021-12-09T14:45:11.740690Z","shell.execute_reply":"2021-12-09T14:45:12.560450Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\n# x y z hypocamp = \n#x = 40, 80   autre  140\n#y = 90, 130  autre 132\n#z = 40, 80   autre 77\nusecols = ['Subject ID', 'Rooster ID', 'Group']\n# ['CN', 'AD']\ndef prepare_X_of_Y_3D(Y):\n    X_data = []\n    Y_data = []\n    X_test_index = []\n    Y_test = []\n    path = \"../input/adni-data/ADNI_PROCESSED\"\n    n_test_AD = 0\n    n_test_CN = 0\n    #f = open(path, \"r\")\n    for index, row in Y.iterrows():\n        file = path + '/n_mmni_fADNI_' + index + '_1.5T_t1w.nii'\n        if os.path.isfile(file):\n            img_n_mmni = nib.load(file)\n            # Taking 4 images for test purpose\n            if (Y['Group'][index] == 'AD' and n_test_AD < 2) or (Y['Group'][index] == 'CN' and n_test_CN < 2):\n                n_test_AD += 1 if Y['Group'][index] == 'AD' else n_test_AD\n                n_test_CN += 1 if Y['Group'][index] == 'CN' else n_test_CN\n                X_test_index.append(index)\n            else:\n                img_data_left_right = []\n                \n                #left hyppocampus\n                img_data = custom_patch_3D(img_n_mmni, x_tup=(40, 80), y_tup=(90, 130), z_tup=(40, 80))\n                img_data_left_right.append(img_data)\n                \n                #right hyppocampus\n                img_data = custom_patch_3D(img_n_mmni, x_tup=(100,140), y_tup=(90, 130), z_tup=(40, 80))\n                img_data_left_right.append(img_data)\n        \n                #append left and right hyppocamus to data\n                X_data.append(img_data_left_right)\n            \n                if Y['Group'][index] == 'AD':\n                    Y_data.append(1)\n                elif Y['Group'][index] == 'CN':\n                    Y_data.append(0)\n        else:\n            Y.drop(index, inplace=True)\n    return np.array(X_data), Y_data, X_test_index\n'''\nif os.path.isfile('X_data_3D.npy') and os.path.isfile('Y_data_3D.npy') and os.path.isfile('X_test_index_3D.npy'):\n    X_data_3D = np.load('X_data_3D.npy')\n    Y_data_3D = np.load('Y_data_3D.npy')\n    X_test_index_3D = np.load('X_test_index_3D.npy')\nelse:\n'''\nX_data_3D, Y_data_list_3D, X_test_index_3D = prepare_X_of_Y_3D(y_data)\nY_data_3D = to_categorical(Y_data_list_3D, num_classes=2)\n    \nprint(len(X_data_3D) == len(Y_data_3D))\nprint(len(X_data_3D))","metadata":{"execution":{"iopub.status.busy":"2021-12-09T14:45:20.425966Z","iopub.execute_input":"2021-12-09T14:45:20.426281Z","iopub.status.idle":"2021-12-09T14:46:26.782331Z","shell.execute_reply.started":"2021-12-09T14:45:20.426248Z","shell.execute_reply":"2021-12-09T14:46:26.781441Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Data augmentation by adding right hyppocampus (mirrored) to data with the left one.","metadata":{"execution":{"iopub.status.busy":"2021-12-09T14:51:39.527635Z","iopub.execute_input":"2021-12-09T14:51:39.528258Z","iopub.status.idle":"2021-12-09T14:51:39.532028Z","shell.execute_reply.started":"2021-12-09T14:51:39.528218Z","shell.execute_reply":"2021-12-09T14:51:39.531137Z"}}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\n# x y z hypocamp = \n#x = 40, 80   autre  140\n#y = 90, 130  autre 132\n#z = 40, 80   autre 77\nusecols = ['Subject ID', 'Rooster ID', 'Group']\n# ['CN', 'AD']\ndef prepare_X_of_Y_3D_left(Y):\n    X_data = []\n    Y_data = []\n    X_test_index = []\n    Y_test = []\n    path = \"../input/adni-data/ADNI_PROCESSED\"\n    n_test_AD = 0\n    n_test_CN = 0\n    #f = open(path, \"r\")\n    for index, row in Y.iterrows():\n        file = path + '/n_mmni_fADNI_' + index + '_1.5T_t1w.nii'\n        if os.path.isfile(file):\n            img_n_mmni = nib.load(file)\n            # Taking 4 images for test purpose\n            if (Y['Group'][index] == 'AD' and n_test_AD < 2) or (Y['Group'][index] == 'CN' and n_test_CN < 2):\n                n_test_AD += 1 if Y['Group'][index] == 'AD' else n_test_AD\n                n_test_CN += 1 if Y['Group'][index] == 'CN' else n_test_CN\n                X_test_index.append(index)\n            else:\n                img_data = custom_patch_3D(img_n_mmni, x_tup=(90,130), y_tup=(90, 130), z_tup=(50, 110))\n                X_data.append(img_data)\n                if Y['Group'][index] == 'AD':\n                    Y_data.append(1)\n                elif Y['Group'][index] == 'CN':\n                    Y_data.append(0)\n        else:\n            Y.drop(index, inplace=True)\n    return np.array(X_data), Y_data, X_test_index\n\ndef prepare_X_of_Y_3D_right(Y):\n    X_data = []\n    Y_data = []\n    X_test_index = []\n    Y_test = []\n    path = \"../input/adni-data/ADNI_PROCESSED\"\n    n_test_AD = 0\n    n_test_CN = 0\n    #f = open(path, \"r\")\n    for index, row in Y.iterrows():\n        file = path + '/n_mmni_fADNI_' + index + '_1.5T_t1w.nii'\n        if os.path.isfile(file):\n            img_n_mmni = nib.load(file)\n            # Taking 4 images for test purpose\n            if (Y['Group'][index] == 'AD' and n_test_AD < 2) or (Y['Group'][index] == 'CN' and n_test_CN < 2):\n                n_test_AD += 1 if Y['Group'][index] == 'AD' else n_test_AD\n                n_test_CN += 1 if Y['Group'][index] == 'CN' else n_test_CN\n                X_test_index.append(index)\n            else:\n                img_data = custom_patch_3D(img_n_mmni, x_tup=(40, 80), y_tup=(90, 130), z_tup=(50, 110))\n                img_data = img_data.reshape((40 ,40, 60, 1))\n                img_data = np.transpose(img_data, (1, 0, 2, 3))\n                img_data = img_data.reshape((40 ,40, 60, 1))\n                \n                X_data.append(img_data)\n                if Y['Group'][index] == 'AD':\n                    Y_data.append(1)\n                elif Y['Group'][index] == 'CN':\n                    Y_data.append(0)\n        else:\n            Y.drop(index, inplace=True)\n    return np.array(X_data), Y_data, X_test_index\n\nX_data_3D_right, Y_data_list_3D_right, X_test_index_3D_right = prepare_X_of_Y_3D_right(y_data)\nX_data_3D_left, Y_data_list_3D_left, X_test_index_3D_left = prepare_X_of_Y_3D_right(y_data)\n\nX_data_3D = np.concatenate((X_data_3D_right, X_data_3D_left))\nX_test_index_3D =Y_data_list_3D = np.concatenate((X_test_index_3D_right, X_test_index_3D_left))\n\nY_data_3D_left = to_categorical(Y_data_list_3D_left, num_classes=2)\nY_data_3D_right = to_categorical(Y_data_list_3D_right, num_classes=2)\n\nY_data_3D = np.concatenate((Y_data_3D_right, Y_data_3D_left))\n\nprint(len(X_data_3D) == len(Y_data_3D))\nprint(len(X_data_3D))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Left Hyppocampus example","metadata":{}},{"cell_type":"code","source":"from nibabel.viewers import OrthoSlicer3D\nOrthoSlicer3D(X_data_3D[1][0]).show()","metadata":{"execution":{"iopub.status.busy":"2021-12-09T14:48:16.320481Z","iopub.execute_input":"2021-12-09T14:48:16.321337Z","iopub.status.idle":"2021-12-09T14:48:16.911292Z","shell.execute_reply.started":"2021-12-09T14:48:16.321299Z","shell.execute_reply":"2021-12-09T14:48:16.910611Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### Right Hyppocampus example","metadata":{}},{"cell_type":"code","source":"from nibabel.viewers import OrthoSlicer3D\nOrthoSlicer3D(X_data_3D[1][1]).show()","metadata":{"execution":{"iopub.status.busy":"2021-12-09T14:48:39.551746Z","iopub.execute_input":"2021-12-09T14:48:39.552005Z","iopub.status.idle":"2021-12-09T14:48:39.976189Z","shell.execute_reply.started":"2021-12-09T14:48:39.551975Z","shell.execute_reply":"2021-12-09T14:48:39.975521Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Loss visualization function\nThis function is used to visualize the variations of loss, val_loss, accuracy and val_accuracy over epochs. It is used for 3D and 2D models. ","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plot the validation and training data separately\ndef plot_loss_curves(history):\n      \"\"\"\n      Returns separate loss curves for training and validation metrics.\n      \"\"\" \n      loss = history.history['loss']\n      val_loss = history.history['val_loss']\n\n      accuracy = history.history['accuracy']\n      val_accuracy = history.history['val_accuracy']\n\n      epochs = range(len(history.history['loss']))\n\n      # Plot loss\n      plt.plot(epochs, loss, label='training_loss')\n      plt.plot(epochs, val_loss, label='val_loss')\n      plt.title('Loss')\n      plt.xlabel('Epochs')\n      plt.legend()\n\n      # Plot accuracy\n      plt.figure()  \n      plt.plot(epochs, accuracy, label='training_accuracy')\n      plt.plot(epochs, val_accuracy, label='val_accuracy')\n      plt.title('Accuracy')\n      plt.xlabel('Epochs')\n      plt.legend()\n      plt.savefig('accuracy.png')","metadata":{"execution":{"iopub.status.busy":"2021-12-09T14:41:24.778362Z","iopub.execute_input":"2021-12-09T14:41:24.778803Z","iopub.status.idle":"2021-12-09T14:41:24.787316Z","shell.execute_reply.started":"2021-12-09T14:41:24.778762Z","shell.execute_reply":"2021-12-09T14:41:24.786400Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### 3D implementation of classification model\n- Creation of an auto-encoder model and an only encoder model.\n- compilation of the model using adam optimizer, and binary_crossentropy loss.- fiting the model using GPU if it exists.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Conv2D, SeparableConv2D, MaxPooling2D, UpSampling2D, Cropping2D, Conv3D, MaxPooling3D, UpSampling3D, Cropping3D, Input, concatenate, Flatten, Dense, Dropout, BatchNormalization, Activation, BatchNormalization, GlobalAveragePooling2D, GlobalAveragePooling3D, add\nfrom tensorflow.keras.models import Model\n\ndef create_Unet_model3D_encoder(input_size, depth=5, padding='valid', nb_class=2):\n    inputs = Input(shape=input_size)\n    x = inputs\n    num_filters = 64\n    for i in range(depth):\n        x = Conv3D(filters=num_filters, kernel_size=(3,3,3), padding=padding)(x)\n        x = BatchNormalization()(x)\n        x = Activation('relu')(x)\n        x = Conv3D(filters=num_filters, kernel_size=(3,3,3), padding=padding)(x)\n        x = BatchNormalization()(x)\n        x = Activation('relu')(x)\n        if i != depth - 1:\n            x = MaxPooling3D(pool_size=(2,2,2), strides=2)(x)\n            num_filters *= 2\n    \n        x = Dropout(0.6)(x)\n    \n    x = Flatten()(x)\n    # x = GlobalAveragePooling2D()(x)\n    x = Dense(512, activation='relu', kernel_regularizer='l2')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n    \n    \n    x = Dense(128, activation='relu', kernel_regularizer='l2')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n    \n    x = Dense(64, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n\n    \n    outputs = Dense(nb_class, activation='softmax')(x)\n\n    return Model(inputs, outputs)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T14:41:24.788896Z","iopub.execute_input":"2021-12-09T14:41:24.789634Z","iopub.status.idle":"2021-12-09T14:41:24.804536Z","shell.execute_reply.started":"2021-12-09T14:41:24.789596Z","shell.execute_reply":"2021-12-09T14:41:24.803789Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Conv2D, SeparableConv2D, MaxPooling2D, UpSampling2D, Cropping2D, Conv3D, MaxPooling3D, UpSampling3D, Cropping3D, Input, Concatenate, Flatten, Dense, Dropout, BatchNormalization, Activation, BatchNormalization, GlobalAveragePooling2D, GlobalAveragePooling3D, add\n\ndef intermediate_network(inputs, i, depth, padding='same'):\n    x = inputs[:,i,:,:,:,:]\n    \n    num_filters = 32\n    for i in range(depth):\n        x = Conv3D(filters=num_filters, kernel_size=(3,3,3), padding=padding)(x)\n        x = BatchNormalization()(x)\n        x = Activation('relu')(x)\n        x = Conv3D(filters=num_filters, kernel_size=(3,3,3), padding=padding)(x)\n        x = BatchNormalization()(x)\n        x = Activation('relu')(x)\n        if i != depth - 1:\n            x = MaxPooling3D(pool_size=(2,2,2), strides=2)(x)\n            num_filters *= 2\n    \n    x = Dropout(0.2)(x)\n    x = Flatten()(x)\n    return x\n\ndef create_model_3D(input_size, depth=2, nb_class=2):\n    inputs = Input(shape=input_size)\n    nb_slices = 2\n    out_list = []\n    for i in range(nb_slices):\n        out_list.append(intermediate_network(inputs=inputs, i=i, depth=depth, padding='same'))\n\n    x = Concatenate()(out_list)\n    x = Flatten()(x)\n    \n    x = Dense(512, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.7)(x)\n    \n    \n    x = Dense(128, activation='relu', kernel_regularizer='l2')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n    \n    x = Dense(64, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.3)(x)\n    \n    outputs = Dense(nb_class, activation='softmax')(x)\n    \n    return Model(inputs, outputs)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T14:42:46.811429Z","iopub.execute_input":"2021-12-09T14:42:46.812219Z","iopub.status.idle":"2021-12-09T14:42:46.825531Z","shell.execute_reply.started":"2021-12-09T14:42:46.812172Z","shell.execute_reply":"2021-12-09T14:42:46.824778Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import tensorflow\n\nOPT    = tensorflow.keras.optimizers.Adam(learning_rate=0.0001)#0.0001\n\nmodel_3 = create_model_3D(X_data_3D[0].shape, depth=2, nb_class=2)\nmodel_3.compile(optimizer=OPT, loss='binary_crossentropy', metrics=['accuracy'])\nmodel_3.summary()\n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-09T15:01:47.988092Z","iopub.execute_input":"2021-12-09T15:01:47.988399Z","iopub.status.idle":"2021-12-09T15:01:48.032481Z","shell.execute_reply.started":"2021-12-09T15:01:47.988364Z","shell.execute_reply":"2021-12-09T15:01:48.031643Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import tensorflow\n\nOPT    = tensorflow.keras.optimizers.Adam(learning_rate=0.0001)#0.0001\n\nmodel_3 = create_Unet_model3D_encoder(X_data_3D[0].shape, depth=2)\nmodel_3.compile(optimizer=OPT, loss='binary_crossentropy', metrics=['accuracy'])\nmodel_3.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_data_3D[1:200].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# reduce amount of data if GPU can't handle: X_data_3D[0:100], Y_data_3D[0:100]\nX_train_3D, X_val_3D, Y_train_3D, Y_val_3D = train_test_split(X_data_3D, Y_data_3D, test_size=0.2,random_state=42)\nprint(\"Data splited\")","metadata":{"execution":{"iopub.status.busy":"2021-12-09T14:43:41.141660Z","iopub.execute_input":"2021-12-09T14:43:41.142139Z","iopub.status.idle":"2021-12-09T14:43:41.183227Z","shell.execute_reply.started":"2021-12-09T14:43:41.142100Z","shell.execute_reply":"2021-12-09T14:43:41.182498Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import EarlyStopping\n\nes = EarlyStopping(monitor='val_accuracy', mode='min', verbose=1, patience=20)\nhistory = model_3.fit(np.asarray(X_train_3D), np.asarray(Y_train_3D), epochs= 15, batch_size=16, validation_data=(X_val_3D, Y_val_3D), callbacks=[es])","metadata":{"execution":{"iopub.status.busy":"2021-12-09T15:01:54.853603Z","iopub.execute_input":"2021-12-09T15:01:54.854291Z","iopub.status.idle":"2021-12-09T15:01:54.879391Z","shell.execute_reply.started":"2021-12-09T15:01:54.854253Z","shell.execute_reply":"2021-12-09T15:01:54.878591Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"plot_loss_curves(history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}