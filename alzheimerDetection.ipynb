{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from nibabel.testing import data_path\n",
    "import nibabel as nib\n",
    "from pathlib import Path\n",
    "\n",
    "path = str(Path().resolve())\n",
    "path = path + \"\\\\ADNI_PROCESSED\"\n",
    "\n",
    "def apply_mask(img_n_mmni, img_mask):\n",
    "    \"\"\"\n",
    "        Taking a n_mmni and apply the correspondant mask\n",
    "        param:\n",
    "            img_n_mmi   : image n_mmi\n",
    "            img_mask    : mask\n",
    "    \"\"\"\n",
    "    mmni_m = img_n_mmni.get_fdata()\n",
    "    mask_m = img_mask.get_fdata().astype(bool)\n",
    "    mask_bg = np.logical_not(mask_m)\n",
    "    mmni_m[mask_bg] = 0\n",
    "    return mmni_m\n",
    "\n",
    "def process_irm_data():\n",
    "    \"\"\"\n",
    "        Create a new directory and process all images from tha ADNI1 directory\n",
    "    \"\"\"\n",
    "    path = str(Path().resolve())\n",
    "    path_res = path + \"\\\\ADNI_PROCESSED\"\n",
    "    Path(path_res).mkdir(parents=True, exist_ok=True) # Create a directory for data processed\n",
    "    path = path + \"\\\\ADNI1\"\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.startswith(\"n_mmni\"):\n",
    "            n_mmni_filename = os.path.join(path, filename)\n",
    "            mask_filename = os.path.join(path, \"mask_\" + filename)\n",
    "            img_n_mmni = nib.load(n_mmni_filename)\n",
    "            img_mask = nib.load(mask_filename)\n",
    "            n_mmni_mask = apply_mask(img_n_mmni, img_mask)\n",
    "            img = nib.Nifti1Image(n_mmni_mask, np.eye(4))\n",
    "            nib.save(img, os.path.join(path_res, filename))\n",
    "\n",
    "# process_irm_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_data(path):\n",
    "    \"\"\"\n",
    "        load all n_mmni found in the path\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(path):\n",
    "        print(\"Can't found directory: \" + path)\n",
    "    else:\n",
    "        list_x = []\n",
    "        for filename in os.listdir(path):\n",
    "            n_mmni_filename = os.path.join(path, filename)\n",
    "            img_n_mmni = nib.load(n_mmni_filename)\n",
    "            mmni_matrix = img_n_mmni.get_fdata()\n",
    "            list_x.append((filename, mmni_matrix))\n",
    "        return list_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(181, 181, 1)\n"
     ]
    }
   ],
   "source": [
    "def cut_2D_i(img_n_mmni, axe, idx):\n",
    "    \"\"\"\n",
    "        Function that returns a 2D cut from the \"img\" in the index \"idx\", along the axe given in parameter\n",
    "    \"\"\"\n",
    "    axe_dim = {\"x\": img_n_mmni.shape[0], \"y\": img_n_mmni.shape[1], \"z\":img_n_mmni.shape[2]}\n",
    "    if axe_dim[axe] <= idx or idx < 0:\n",
    "        print(\"Invalid value for index must be between 0 and \" , axe_dim[axe])\n",
    "        return\n",
    "    if axe == \"x\":\n",
    "        cropped_img = img_n_mmni.slicer[idx:idx+1, 18:199, :]\n",
    "        img_data = cropped_img.get_fdata()\n",
    "        img_data = np.transpose(img_data, (2, 1, 0)) / 255.0\n",
    "        img_data = np.transpose(img_data, (1, 0, 2)) / 255.0\n",
    "    elif axe == \"y\":\n",
    "        cropped_img = img_n_mmni.slicer[:, idx:idx+1,:]\n",
    "        img_data = cropped_img.get_fdata()\n",
    "        img_data = np.transpose(img_data, (0, 2, 1)) / 255.0\n",
    "    elif axe == \"z\":\n",
    "        cropped_img = img_n_mmni.slicer[:, 18:199, idx:idx+1]\n",
    "        img_data = cropped_img.get_fdata()\n",
    "    else:\n",
    "        print(\"Choose a valid value for axe: x, y or z\")\n",
    "\n",
    "    return img_data\n",
    "\n",
    "def custom_patch_3D(img_n_mmni, x_tup, y_tup, z_tup):\n",
    "    axe_dim = {\"x\": img_n_mmni.shape[0], \"y\": img_n_mmni.shape[1], \"z\":img_n_mmni.shape[2]}\n",
    "    if axe_dim[\"x\"] <= x_tup[1] or x_tup[0] < 0 or axe_dim[\"y\"] <= y_tup[1] or y_tup[0] < 0 or axe_dim[\"z\"] <= z_tup[1] or z_tup[0] < 0 :\n",
    "        print(\"Invalid values\")\n",
    "        return    \n",
    "    else:\n",
    "        cropped_img = img_n_mmni.slicer[x_tup[0]:x_tup[1], y_tup[0]:y_tup[1], z_tup[0]:z_tup[1]]\n",
    "        return cropped_img\n",
    "\n",
    "# To test thid function\n",
    "n_mmni_filename = os.path.join(path, \"n_mmni_fADNI_002_S_0295_1.5T_t1w.nii.gz\")\n",
    "img_n_mmni = nib.load(n_mmni_filename)\n",
    "data_img = cut_2D_i(img_n_mmni, \"x\", 95)\n",
    "print(data_img.shape)\n",
    "crop_img = nib.Nifti1Image(data_img, np.eye(4))\n",
    "nib.save(crop_img, 'test_img.nii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOT USED\n",
    "def load_X_data(path):\n",
    "    if not os.path.isdir(path):\n",
    "        print(\"Can't found directory: \" + path)\n",
    "    else:\n",
    "        list_x = []\n",
    "        for filename in os.listdir(path):\n",
    "            n_mmni_filename = os.path.join(path, filename)\n",
    "            img_n_mmni = nib.load(n_mmni_filename)\n",
    "            # Customize your choice: taking a 2D cuts or 3D patches\n",
    "            cropped_img = cut_2D_i(img_n_mmni, \"y\", 106)\n",
    "            cropped_n_mmni_matrix = cropped_img.get_fdata()\n",
    "            list_x.append((filename, cropped_n_mmni_matrix))\n",
    "        return list_x\n",
    "\n",
    "# X_data = load_X_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rooster ID</th>\n",
       "      <th>Group</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subject ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>002_S_0295</th>\n",
       "      <td>295</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>002_S_0413</th>\n",
       "      <td>413</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>002_S_0559</th>\n",
       "      <td>559</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>002_S_0619</th>\n",
       "      <td>619</td>\n",
       "      <td>AD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>002_S_0685</th>\n",
       "      <td>685</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>002_S_0816</th>\n",
       "      <td>816</td>\n",
       "      <td>AD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>002_S_0938</th>\n",
       "      <td>938</td>\n",
       "      <td>AD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Rooster ID Group\n",
       "Subject ID                  \n",
       "002_S_0295         295    CN\n",
       "002_S_0413         413    CN\n",
       "002_S_0559         559    CN\n",
       "002_S_0619         619    AD\n",
       "002_S_0685         685    CN\n",
       "002_S_0816         816    AD\n",
       "002_S_0938         938    AD"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_data(path):\n",
    "    data = pd.read_csv(path, names= ['Subject ID', 'Rooster ID', 'Age', 'Sexe', 'Group', 'Conversion', 'MMSE', 'RAVLT', 'FAQ', 'CDR-SB', 'ADAS11'], usecols = ['Subject ID', 'Rooster ID', 'Group'])\n",
    "    data.index = data['Subject ID']\n",
    "    data = data.drop(['Subject ID'], axis=1)\n",
    "    data = data[(data.Group == 'CN') | (data.Group == 'AD')]\n",
    "    return data\n",
    "\n",
    "path = str(Path().resolve())\n",
    "path = path + \"/ADNI1/list_standardized_tongtong_2017.csv\"\n",
    "y_data = load_data(path)\n",
    "y_data.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "usecols = ['Subject ID', 'Rooster ID', 'Group']\n",
    "# ['CN', 'AD']\n",
    "def prepare_X_of_Y(Y):\n",
    "    X_data = []\n",
    "    Y_data = []\n",
    "    X_test_index = []\n",
    "    Y_test = []\n",
    "    path = str(Path().resolve())\n",
    "    path += \"\\\\ADNI_PROCESSED\"\n",
    "    n_test_AD = 0\n",
    "    n_test_CN = 0\n",
    "    for index, row in Y.iterrows():\n",
    "        file = path + '\\\\n_mmni_fADNI_' + index + '_1.5T_t1w.nii.gz'\n",
    "        if os.path.isfile(file):\n",
    "            img_n_mmni = nib.load(file)\n",
    "            # Taking 4 images for test purpose\n",
    "            if (Y['Group'][index] == 'AD' and n_test_AD < 2) or (Y['Group'][index] == 'CN' and n_test_CN < 2):\n",
    "                n_test_AD += 1 if Y['Group'][index] == 'AD' else n_test_AD\n",
    "                n_test_CN += 1 if Y['Group'][index] == 'CN' else n_test_CN\n",
    "                X_test_index.append(index)\n",
    "            else:\n",
    "                for axe in ['x', 'y', 'z']:\n",
    "                    if axe =='z':\n",
    "                        slice_range = range(72, 88)\n",
    "                    #elif axe == 'x':\n",
    "                    #    slice_range = range(101, 107)\n",
    "                    else:\n",
    "                        slice_range = range(101, 107)\n",
    "\n",
    "                    for i in slice_range:\n",
    "                        img_data = cut_2D_i(img_n_mmni, axe, i)\n",
    "                        X_data.append(img_data)\n",
    "                        if Y['Group'][index] == 'AD':\n",
    "                            Y_data.append(1)\n",
    "                        elif Y['Group'][index] == 'CN':\n",
    "                            Y_data.append(0)\n",
    "        else:\n",
    "            Y.drop(index, inplace=True)\n",
    "    return np.array(X_data), Y_data, X_test_index\n",
    "\n",
    "if os.path.isfile('X_data.npy') and os.path.isfile('Y_data.npy') and os.path.isfile('X_test_index.npy'):\n",
    "    X_data = np.load('X_data.npy')\n",
    "    Y_data = np.load('Y_data.npy')\n",
    "    X_test_index = np.load('X_test_index.npy')\n",
    "else:\n",
    "    X_data, Y_data_list, X_test_index = prepare_X_of_Y(y_data)\n",
    "    Y_data = to_categorical(Y_data_list, num_classes=2)\n",
    "    np.save('X_data', X_data)\n",
    "    np.save('Y_data', Y_data)\n",
    "    np.save('X_test_index', X_test_index)\n",
    "    \n",
    "print(len(X_data) == len(Y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y_data_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19520/141507320.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"labels\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY_data_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mlab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'labels'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Y_data_list' is not defined"
     ]
    }
   ],
   "source": [
    "map_characters = {0: 'CN', 1: 'AD'}\n",
    "dict_characters=map_characters\n",
    "import seaborn as sns\n",
    "df = pd.DataFrame()\n",
    "df[\"labels\"] = Y_data_list\n",
    "lab = df['labels']\n",
    "dist = lab.value_counts()\n",
    "sns.countplot(lab)\n",
    "print(dict_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net Neural Network\n",
    "\n",
    "Creation of two U-Net models:\n",
    "* One for 2D inputs, in case we slice the input into 2D images.\n",
    "* The other for 3D inputs, in case we use 3D blocs of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, SeparableConv2D, MaxPooling2D, UpSampling2D, Cropping2D, Conv3D, MaxPooling3D, UpSampling3D, Cropping3D, Input, concatenate, Flatten, Dense, Dropout, BatchNormalization, Activation, BatchNormalization, GlobalAveragePooling2D, GlobalAveragePooling3D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "def create_Unet_model2D(input_size, depth=5, padding='valid'):\n",
    "    inputs = Input(shape=input_size)\n",
    "    x = inputs\n",
    "    num_filters = 64\n",
    "    encode_layers_list = []\n",
    "    for i in range(depth):\n",
    "        x = Conv2D(filters=num_filters, kernel_size=(3,3), padding=padding, activation='relu')(x)\n",
    "        x = Conv2D(filters=num_filters, kernel_size=(3,3), padding=padding, activation='relu')(x)\n",
    "        if i != depth - 1:\n",
    "            encode_layers_list.append(x)\n",
    "            x = MaxPooling2D(pool_size=(2,2), strides=2)(x)\n",
    "            num_filters *= 2\n",
    "    \n",
    "    for i in range(depth - 1):\n",
    "        x = UpSampling2D()(x)\n",
    "        x = Conv2D(filters=num_filters, kernel_size=(2,2), padding='same', activation='relu')(x)\n",
    "        #cropping\n",
    "        encoder_shape = encode_layers_list[depth - 2 - i].shape\n",
    "        decoder_shape = x.shape\n",
    "        shape_diff = (encoder_shape[1] - decoder_shape[1], encoder_shape[2] - decoder_shape[2])\n",
    "        xshape_diff = shape_diff[0] // 2\n",
    "        yshape_diff = shape_diff[1] // 2\n",
    "        if shape_diff[0] % 2 != 0:\n",
    "            xshape_diff = (shape_diff[0] // 2, shape_diff[0] // 2 + 1)\n",
    "        if shape_diff[1] % 2 != 0:\n",
    "            yshape_diff = (shape_diff[1] // 2, shape_diff[1] // 2 + 1)\n",
    "        croped_layer = Cropping2D(cropping=(xshape_diff, yshape_diff))(encode_layers_list[depth - 2 - i])\n",
    "        \n",
    "        x = concatenate([croped_layer, x])\n",
    "        num_filters /= 2\n",
    "        x = Conv2D(filters=num_filters, kernel_size=(3,3), padding=padding, activation='relu')(x)\n",
    "        x = Conv2D(filters=num_filters, kernel_size=(3,3), padding=padding, activation='relu')(x)\n",
    "\n",
    "    outputs = Conv2D(filters=2, padding=padding, kernel_size=(1,1))(x)\n",
    "        \n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "def create_Unet_model3D(input_size, depth=5, padding='valid'):\n",
    "    inputs = Input(shape=input_size)\n",
    "    x = inputs\n",
    "    num_filters = 64\n",
    "    encode_layers_list = []\n",
    "    for i in range(depth):\n",
    "        x = Conv3D(filters=num_filters, kernel_size=(3,3,3), padding=padding, activation='relu')(x)\n",
    "        x = Conv3D(filters=num_filters, kernel_size=(3,3,3), padding=padding, activation='relu')(x)\n",
    "        if i != depth - 1:\n",
    "            encode_layers_list.append(x)\n",
    "            x = MaxPooling3D(pool_size=(2,2,2), strides=2)(x)\n",
    "            num_filters *= 2\n",
    "    \n",
    "\n",
    "    for i in range(depth - 1):\n",
    "        x = UpSampling3D()(x)\n",
    "        x = Conv3D(filters=num_filters, kernel_size=(2,2,2), activation='relu', padding='same')(x)\n",
    "        #cropping\n",
    "        encoder_shape = encode_layers_list[depth - 2 - i].shape\n",
    "        decoder_shape = x.shape\n",
    "        shape_diff = (encoder_shape[1] - decoder_shape[1], encoder_shape[2] - decoder_shape[2], encoder_shape[3] - decoder_shape[3])\n",
    "        xshape_diff = shape_diff[0] // 2\n",
    "        yshape_diff = shape_diff[1] // 2\n",
    "        zshape_diff = shape_diff[2] // 2\n",
    "        if shape_diff[0] % 2 != 0:\n",
    "            xshape_diff = (shape_diff[0] // 2, shape_diff[0] // 2 + 1)\n",
    "        if shape_diff[1] % 2 != 0:\n",
    "            yshape_diff = (shape_diff[1] // 2, shape_diff[1] // 2 + 1)\n",
    "        if shape_diff[2] % 2 != 0:\n",
    "            zshape_diff = (shape_diff[2] // 2, shape_diff[2] // 2 + 1)\n",
    "        croped_layer = Cropping3D(cropping=(xshape_diff, yshape_diff, zshape_diff))(encode_layers_list[depth - 2 - i])\n",
    "\n",
    "        x = concatenate([croped_layer, x])\n",
    "        num_filters /= 2\n",
    "        x = Conv3D(filters=num_filters, kernel_size=(3,3,3), padding=padding, activation='relu')(x)\n",
    "        x = Conv3D(filters=num_filters, kernel_size=(3,3,3), padding=padding, activation='relu')(x)\n",
    "\n",
    "    outputs = Conv3D(filters=2, kernel_size=(1,1,1))(x)\n",
    "        \n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "def create_Unet_model2D_encoder(input_size, depth=5, padding='valid', nb_class=2):\n",
    "    inputs = Input(shape=input_size)\n",
    "    x = inputs\n",
    "    num_filters = 64\n",
    "    for i in range(depth):\n",
    "        x = SeparableConv2D(filters=num_filters, kernel_size=(3,3), padding=padding)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = SeparableConv2D(filters=num_filters, kernel_size=(3,3), padding=padding)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        if i != depth - 1:\n",
    "            x = MaxPooling2D(pool_size=(2,2), strides=2)(x)\n",
    "            num_filters *= 2\n",
    "\n",
    "    # Flattenting\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(nb_class, activation='sigmoid')(x)\n",
    "\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "def create_Unet_model3D_encoder(input_size, depth=5, padding='valid', nb_class=2):\n",
    "    inputs = Input(shape=input_size)\n",
    "    x = inputs\n",
    "    num_filters = 64\n",
    "    for i in range(depth):\n",
    "        x = Conv3D(filters=num_filters, kernel_size=(3,3,3), padding=padding)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Conv3D(filters=num_filters, kernel_size=(3,3,3), padding=padding)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        if i != depth - 1:\n",
    "            x = MaxPooling3D(pool_size=(2,2,2), strides=2)(x)\n",
    "            num_filters *= 2\n",
    "    \n",
    "    x = GlobalAveragePooling3D()(x)\n",
    "\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(nb_class, activation='sigmoid')(x)\n",
    "\n",
    "    return Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 181, 181, 1)]     0         \n",
      "                                                                 \n",
      " separable_conv2d (Separable  (None, 179, 179, 64)     137       \n",
      " Conv2D)                                                         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 179, 179, 64)     256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 179, 179, 64)      0         \n",
      "                                                                 \n",
      " separable_conv2d_1 (Separab  (None, 177, 177, 64)     4736      \n",
      " leConv2D)                                                       \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 177, 177, 64)     256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 177, 177, 64)      0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 88, 88, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " separable_conv2d_2 (Separab  (None, 86, 86, 128)      8896      \n",
      " leConv2D)                                                       \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 86, 86, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 86, 86, 128)       0         \n",
      "                                                                 \n",
      " separable_conv2d_3 (Separab  (None, 84, 84, 128)      17664     \n",
      " leConv2D)                                                       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 84, 84, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 84, 84, 128)       0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 42, 42, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " separable_conv2d_4 (Separab  (None, 40, 40, 256)      34176     \n",
      " leConv2D)                                                       \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 40, 40, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 40, 40, 256)       0         \n",
      "                                                                 \n",
      " separable_conv2d_5 (Separab  (None, 38, 38, 256)      68096     \n",
      " leConv2D)                                                       \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 38, 38, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 38, 38, 256)       0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 19, 19, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " separable_conv2d_6 (Separab  (None, 17, 17, 512)      133888    \n",
      " leConv2D)                                                       \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 17, 17, 512)      2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 17, 17, 512)       0         \n",
      "                                                                 \n",
      " separable_conv2d_7 (Separab  (None, 15, 15, 512)      267264    \n",
      " leConv2D)                                                       \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 15, 15, 512)      2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 15, 15, 512)       0         \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 512)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 543,563\n",
      "Trainable params: 539,723\n",
      "Non-trainable params: 3,840\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2 = create_Unet_model2D_encoder(X_data[0].shape, depth=4)\n",
    "# model_2.compile(optimizer='adam', loss='squared_hinge', metrics=['accuracy'])\n",
    "model_2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "y = np.array([y[1] for y in Y_data])\n",
    "x = np.array([tf.image.grayscale_to_rgb(tf.convert_to_tensor(img), name=None) for img in X_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data splited\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_data, Y_data, test_size=0.25,random_state=42)\n",
    "print(\"Data splited\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def show_batch(image_batch, label_X):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for n in range(25):\n",
    "        ax = plt.subplot(5,5,n+1)\n",
    "        plt.imshow(image_batch[n])\n",
    "        if label_X[n]:\n",
    "            plt.title(\"AD\")\n",
    "        else:\n",
    "            plt.title(\"CN\")\n",
    "        plt.axis(\"off\")\n",
    "show_batch(X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "11620\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices()\n",
    "print(physical_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "273/273 [==============================] - 101s 336ms/step - loss: 0.6745 - accuracy: 0.5885 - val_loss: 0.6903 - val_accuracy: 0.5473\n",
      "Epoch 2/20\n",
      "  6/273 [..............................] - ETA: 1:26 - loss: 0.6324 - accuracy: 0.6250"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model/batch_normalization_7/FusedBatchNormV3\n (defined at c:\\python39\\lib\\site-packages\\keras\\layers\\normalization\\batch_normalization.py:589)\n]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_2917]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node model/batch_normalization_7/FusedBatchNormV3:\nIn[0] model/separable_conv2d_7/BiasAdd (defined at c:\\python39\\lib\\site-packages\\keras\\layers\\convolutional.py:2263)\t\nIn[1] model/batch_normalization_7/ReadVariableOp:\t\nIn[2] model/batch_normalization_7/ReadVariableOp_1:\t\nIn[3] model/batch_normalization_7/FusedBatchNormV3/ReadVariableOp:\t\nIn[4] model/batch_normalization_7/FusedBatchNormV3/ReadVariableOp_1:\n\nOperation defined at: (most recent call last)\n>>>   File \"c:\\python39\\lib\\runpy.py\", line 197, in _run_module_as_main\n>>>     return _run_code(code, main_globals, None,\n>>> \n>>>   File \"c:\\python39\\lib\\runpy.py\", line 87, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"c:\\python39\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"c:\\python39\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"c:\\python39\\lib\\asyncio\\events.py\", line 80, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 457, in dispatch_queue\n>>>     await self.process_one()\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 446, in process_one\n>>>     await dispatch(*args)\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 353, in dispatch_shell\n>>>     await result\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 648, in execute_request\n>>>     reply_content = await reply_content\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 353, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n>>>     return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2901, in run_cell\n>>>     result = self._run_cell(\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2947, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3172, in run_cell_async\n>>>     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3364, in run_ast_nodes\n>>>     if (await self.run_code(code, result,  async_=asy)):\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3444, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"C:\\Users\\Appolyon\\AppData\\Local\\Temp/ipykernel_19520/1607988456.py\", line 1, in <module>\n>>>     model_2.fit(X_train, Y_train, epochs= 20, validation_data=(X_val,Y_val))\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\keras\\engine\\training.py\", line 808, in train_step\n>>>     y_pred = self(x, training=True)\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\keras\\engine\\functional.py\", line 451, in call\n>>>     return self._run_internal_graph(\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\keras\\engine\\functional.py\", line 589, in _run_internal_graph\n>>>     outputs = node.layer(*args, **kwargs)\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\keras\\layers\\normalization\\batch_normalization.py\", line 767, in call\n>>>     outputs = self._fused_batch_norm(inputs, training=training)\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\keras\\layers\\normalization\\batch_normalization.py\", line 623, in _fused_batch_norm\n>>>     output, mean, variance = control_flow_util.smart_cond(\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\keras\\utils\\control_flow_util.py\", line 105, in smart_cond\n>>>     return tf.__internal__.smart_cond.smart_cond(\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\keras\\layers\\normalization\\batch_normalization.py\", line 589, in _fused_batch_norm_training\n>>>     return tf.compat.v1.nn.fused_batch_norm(\n>>> ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19520/1607988456.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     60\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model/batch_normalization_7/FusedBatchNormV3\n (defined at c:\\python39\\lib\\site-packages\\keras\\layers\\normalization\\batch_normalization.py:589)\n]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_2917]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node model/batch_normalization_7/FusedBatchNormV3:\nIn[0] model/separable_conv2d_7/BiasAdd (defined at c:\\python39\\lib\\site-packages\\keras\\layers\\convolutional.py:2263)\t\nIn[1] model/batch_normalization_7/ReadVariableOp:\t\nIn[2] model/batch_normalization_7/ReadVariableOp_1:\t\nIn[3] model/batch_normalization_7/FusedBatchNormV3/ReadVariableOp:\t\nIn[4] model/batch_normalization_7/FusedBatchNormV3/ReadVariableOp_1:\n\nOperation defined at: (most recent call last)\n>>>   File \"c:\\python39\\lib\\runpy.py\", line 197, in _run_module_as_main\n>>>     return _run_code(code, main_globals, None,\n>>> \n>>>   File \"c:\\python39\\lib\\runpy.py\", line 87, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"c:\\python39\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"c:\\python39\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"c:\\python39\\lib\\asyncio\\events.py\", line 80, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 457, in dispatch_queue\n>>>     await self.process_one()\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 446, in process_one\n>>>     await dispatch(*args)\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 353, in dispatch_shell\n>>>     await result\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 648, in execute_request\n>>>     reply_content = await reply_content\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 353, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n>>>     return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2901, in run_cell\n>>>     result = self._run_cell(\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2947, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3172, in run_cell_async\n>>>     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3364, in run_ast_nodes\n>>>     if (await self.run_code(code, result,  async_=asy)):\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3444, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"C:\\Users\\Appolyon\\AppData\\Local\\Temp/ipykernel_19520/1607988456.py\", line 1, in <module>\n>>>     model_2.fit(X_train, Y_train, epochs= 20, validation_data=(X_val,Y_val))\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\keras\\engine\\training.py\", line 808, in train_step\n>>>     y_pred = self(x, training=True)\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\keras\\engine\\functional.py\", line 451, in call\n>>>     return self._run_internal_graph(\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\keras\\engine\\functional.py\", line 589, in _run_internal_graph\n>>>     outputs = node.layer(*args, **kwargs)\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\keras\\layers\\normalization\\batch_normalization.py\", line 767, in call\n>>>     outputs = self._fused_batch_norm(inputs, training=training)\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\keras\\layers\\normalization\\batch_normalization.py\", line 623, in _fused_batch_norm\n>>>     output, mean, variance = control_flow_util.smart_cond(\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\keras\\utils\\control_flow_util.py\", line 105, in smart_cond\n>>>     return tf.__internal__.smart_cond.smart_cond(\n>>> \n>>>   File \"c:\\python39\\lib\\site-packages\\keras\\layers\\normalization\\batch_normalization.py\", line 589, in _fused_batch_norm_training\n>>>     return tf.compat.v1.nn.fused_batch_norm(\n>>> "
     ]
    }
   ],
   "source": [
    "model_2.fit(X_train, Y_train, epochs= 20, validation_data=(X_val,Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_X = []\n",
    "path = str(Path().resolve())\n",
    "path = path + \"\\\\ADNI_PROCESSED\"\n",
    "for index in X_test_index:\n",
    "    file = path + '\\\\n_mmni_fADNI_' + index + '_1.5T_t1w.nii.gz'\n",
    "    if os.path.isfile(file):\n",
    "        img_n_mmni = nib.load(file)\n",
    "        crop_img = cut_2D_i(img_n_mmni, \"y\", 104)\n",
    "        img_data = crop_img.get_fdata()\n",
    "        img_data = np.transpose(img_data, (0, 2, 1))/255.0\n",
    "        final_test_X.append(img_data)\n",
    "final_test_X = np.array(final_test_X)\n",
    "y_pred = model_2.predict(final_test_X)\n",
    "plt.figure(figsize=(10,10))\n",
    "for n in range(len(X_test_index)):\n",
    "    ax = plt.subplot(2,2,n+1)\n",
    "    plt.imshow(final_test_X[n])\n",
    "    title = \"Probability of: \" + str(y_pred[n][0]*100) + \"% AD\"\n",
    "    plt.title(title)\n",
    "\n",
    "for n in range(len(X_test_index)):\n",
    "    print(\"IRM Correspond to : \" + y_data['Group'][X_test_index[n]] + \", Prediction: \" + str(y_pred[n][0]*100) + \"% AD\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
